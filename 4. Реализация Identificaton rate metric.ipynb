{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "18fb6086",
   "metadata": {
    "id": "18fb6086"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "b2766059",
   "metadata": {
    "id": "b2766059"
   },
   "outputs": [],
   "source": [
    "def compute_cosine_query_pos(query_dict, query_img_names, query_embeddings):\n",
    "    '''\n",
    "    compute cosine similarities between positive pairs from query (stage 1)\n",
    "    params:\n",
    "    query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n",
    "                the dataset. Value: images corresponding to that class\n",
    "    query_img_names: list of images names\n",
    "    query_embeddings: list of embeddings corresponding to query_img_names\n",
    "    output:\n",
    "    list of floats: similarities between embeddings corresponding\n",
    "                    to the same people from query list\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    result = []\n",
    "    query_img_names = np.array(query_img_names)\n",
    "    query_embeddings = torch.tensor(query_embeddings)\n",
    "\n",
    "    for cls, img_names in query_dict.items():\n",
    "        indices = np.where(np.isin(query_img_names, img_names))[0]\n",
    "        cl_embeddings = query_embeddings[indices]\n",
    "        cl_embeddings = F.normalize(cl_embeddings, p=2, dim=1)\n",
    "        sim_matrix = torch.mm(cl_embeddings, cl_embeddings.T)\n",
    "\n",
    "        num_imgs = sim_matrix.size(0)\n",
    "        mask = torch.triu(torch.ones(num_imgs, num_imgs), diagonal=1).bool()\n",
    "        pos_sims = sim_matrix[mask]\n",
    "\n",
    "        result.extend(pos_sims.tolist())\n",
    "\n",
    "    return result\n",
    "\n",
    "def compute_cosine_query_neg(query_dict, query_img_names, query_embeddings):\n",
    "    '''\n",
    "    compute cosine similarities between negative pairs from query (stage 2)\n",
    "    params:\n",
    "    query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n",
    "                the dataset. Value: images corresponding to that class\n",
    "    query_img_names: list of images names\n",
    "    query_embeddings: list of embeddings corresponding to query_img_names\n",
    "    output:\n",
    "    list of floats: similarities between embeddings corresponding\n",
    "                    to different people from query list\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    result = []\n",
    "    query_img_names = np.array(query_img_names)\n",
    "    query_embeddings = torch.tensor(query_embeddings)\n",
    "\n",
    "    for cl1, cl2 in combinations(query_dict.keys(), 2):\n",
    "        img_names1 = query_dict[cl1]\n",
    "        img_names2 = query_dict[cl2]\n",
    "\n",
    "        indices1 = np.where(np.isin(query_img_names, img_names1))[0]\n",
    "        indices2 = np.where(np.isin(query_img_names, img_names2))[0]\n",
    "\n",
    "        cl_embeddings1 = query_embeddings[indices1]\n",
    "        cl_embeddings2 = query_embeddings[indices2]\n",
    "\n",
    "        cl_embeddings1 = F.normalize(cl_embeddings1, p=2, dim=1)\n",
    "        cl_embeddings2 = F.normalize(cl_embeddings2, p=2, dim=1)\n",
    "        sim_matrix = torch.mm(cl_embeddings1, cl_embeddings2.T)\n",
    "\n",
    "        result.extend(sim_matrix.flatten().tolist())\n",
    "\n",
    "    return result\n",
    "\n",
    "def compute_cosine_query_distractors(query_embeddings, distractors_embeddings):\n",
    "    '''\n",
    "    compute cosine similarities between negative pairs from query and distractors\n",
    "    (stage 3)\n",
    "    params:\n",
    "    query_embeddings: list of embeddings corresponding to query_img_names\n",
    "    distractors_embeddings: list of embeddings corresponding to distractors_img_names\n",
    "    output:\n",
    "    list of floats: similarities between pairs of people (q, d), where q is\n",
    "                    embedding corresponding to photo from query, d —\n",
    "                    embedding corresponding to photo from distractors\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    result = []\n",
    "    query_embeddings = torch.tensor(query_embeddings)\n",
    "    distractors_embeddings = torch.tensor(distractors_embeddings)\n",
    "\n",
    "\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "    distractors_embeddings = F.normalize(distractors_embeddings, p=2, dim=1)\n",
    "    sim_matrix = torch.mm(query_embeddings, distractors_embeddings.T)\n",
    "\n",
    "    result.extend(sim_matrix.flatten().tolist())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "b45425e1",
   "metadata": {
    "id": "b45425e1"
   },
   "outputs": [],
   "source": [
    "test_query_dict = {\n",
    "    2876: ['1.jpg', '2.jpg', '3.jpg'],\n",
    "    5674: ['5.jpg'],\n",
    "    864:  ['9.jpg', '10.jpg'],\n",
    "}\n",
    "test_query_img_names = ['1.jpg', '2.jpg', '3.jpg', '5.jpg', '9.jpg', '10.jpg']\n",
    "test_query_embeddings = [\n",
    "                    [1.56, 6.45,  -7.68],\n",
    "                    [-1.1 , 6.11,  -3.0],\n",
    "                    [-0.06,-0.98,-1.29],\n",
    "                    [8.56, 1.45,  1.11],\n",
    "                    [0.7,  1.1,   -7.56],\n",
    "                    [0.05, 0.9,   -2.56],\n",
    "]\n",
    "\n",
    "test_distractors_img_names = ['11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg']\n",
    "\n",
    "test_distractors_embeddings = [\n",
    "                    [0.12, -3.23, -5.55],\n",
    "                    [-1,   -0.01, 1.22],\n",
    "                    [0.06, -0.23, 1.34],\n",
    "                    [-6.6, 1.45,  -1.45],\n",
    "                    [0.89,  1.98, 1.45],\n",
    "]\n",
    "\n",
    "test_cosine_query_pos = compute_cosine_query_pos(test_query_dict, test_query_img_names,\n",
    "                                            test_query_embeddings)\n",
    "test_cosine_query_neg = compute_cosine_query_neg(test_query_dict, test_query_img_names,\n",
    "                                            test_query_embeddings)\n",
    "test_cosine_query_distractors = compute_cosine_query_distractors(test_query_embeddings,\n",
    "                                                            test_distractors_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "41fbcb87",
   "metadata": {
    "id": "41fbcb87"
   },
   "outputs": [],
   "source": [
    "true_cosine_query_pos = [0.8678237233650096, 0.21226104378511604,\n",
    "                         -0.18355866977496182, 0.9787437979250561]\n",
    "assert np.allclose(sorted(test_cosine_query_pos), sorted(true_cosine_query_pos)), \\\n",
    "      \"A mistake in compute_cosine_query_pos function\"\n",
    "\n",
    "true_cosine_query_neg = [0.15963231223161822, 0.8507997093616965, 0.9272761484302097,\n",
    "                         -0.0643994061127092, 0.5412660901220571, 0.701307100338029,\n",
    "                         -0.2372575528216902, 0.6941032794522218, 0.549425446066643,\n",
    "                         -0.011982733001947084, -0.0466679194884999]\n",
    "assert np.allclose(sorted(test_cosine_query_neg), sorted(true_cosine_query_neg)), \\\n",
    "      \"A mistake in compute_cosine_query_neg function\"\n",
    "\n",
    "true_cosine_query_distractors = [0.3371426578637511, -0.6866465610863652, -0.8456563512871669,\n",
    "                                 0.14530087113136106, 0.11410510307646118, -0.07265097629002357,\n",
    "                                 -0.24097699660707042,-0.5851992679925766, 0.4295494455718534,\n",
    "                                 0.37604478596058194, 0.9909483738948858, -0.5881093317868022,\n",
    "                                 -0.6829712976642919, 0.07546364489032083, -0.9130970963915521,\n",
    "                                 -0.17463101988684684, -0.5229363015558941, 0.1399896725311533,\n",
    "                                 -0.9258034013399499, 0.5295114163723346, 0.7811585442749943,\n",
    "                                 -0.8208760031249596, -0.9905139680301821, 0.14969764653247228,\n",
    "                                 -0.40749654525418444, 0.648660814944824, -0.7432584300096284,\n",
    "                                 -0.9839696492435877, 0.2498741082804709, -0.2661183373780491]\n",
    "assert np.allclose(sorted(test_cosine_query_distractors), sorted(true_cosine_query_distractors)), \\\n",
    "      \"A mistake in compute_cosine_query_distractors function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b7c0491c",
   "metadata": {
    "id": "b7c0491c"
   },
   "outputs": [],
   "source": [
    "test_distractors_img_names = ['11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg']\n",
    "\n",
    "test_distractors_embeddings = [\n",
    "                    [0.12, -3.23, -5.55],\n",
    "                    [-1,   -0.01, 1.22],\n",
    "                    [0.06, -0.23, 1.34],\n",
    "                    [-6.6, 1.45,  -1.45],\n",
    "                    [0.89,  1.98, 1.45],\n",
    "]\n",
    "test_cosine_query_distractors = compute_cosine_query_distractors(test_query_embeddings,\n",
    "                                                            test_distractors_embeddings)\n",
    "true_cosine_query_distractors = [0.3371426578637511, -0.6866465610863652, -0.8456563512871669,\n",
    "                                 0.14530087113136106, 0.11410510307646118, -0.07265097629002357,\n",
    "                                 -0.24097699660707042,-0.5851992679925766, 0.4295494455718534,\n",
    "                                 0.37604478596058194, 0.9909483738948858, -0.5881093317868022,\n",
    "                                 -0.6829712976642919, 0.07546364489032083, -0.9130970963915521,\n",
    "                                 -0.17463101988684684, -0.5229363015558941, 0.1399896725311533,\n",
    "                                 -0.9258034013399499, 0.5295114163723346, 0.7811585442749943,\n",
    "                                 -0.8208760031249596, -0.9905139680301821, 0.14969764653247228,\n",
    "                                 -0.40749654525418444, 0.648660814944824, -0.7432584300096284,\n",
    "                                 -0.9839696492435877, 0.2498741082804709, -0.2661183373780491]\n",
    "assert np.allclose(sorted(test_cosine_query_distractors), sorted(true_cosine_query_distractors)), \\\n",
    "      \"A mistake in compute_cosine_query_distractors function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "c1c17788",
   "metadata": {
    "id": "c1c17788"
   },
   "outputs": [],
   "source": [
    "def compute_ir(cosine_query_pos, cosine_query_neg, cosine_query_distractors,\n",
    "               fpr=0.1):\n",
    "    '''\n",
    "    compute identification rate using precomputer cosine similarities between pairs\n",
    "    at given fpr\n",
    "    params:\n",
    "    cosine_query_pos: cosine similarities between positive pairs from query\n",
    "    cosine_query_neg: cosine similarities between negative pairs from query\n",
    "    cosine_query_distractors: cosine similarities between negative pairs\n",
    "                              from query and distractors\n",
    "    fpr: false positive rate at which to compute TPR\n",
    "    output:\n",
    "    float: threshold for given fpr\n",
    "    float: TPR at given FPR\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    false_distances = cosine_query_neg + cosine_query_distractors\n",
    "    false_distances_count = len(false_distances)\n",
    "    N = int(false_distances_count * fpr)\n",
    "    sorted_false_distancess = sorted(false_distances, reverse=True)\n",
    "    threshold = sorted_false_distancess[N]\n",
    "    true_distances_count = len(cosine_query_pos)\n",
    "    cosine_query_pos = np.array(cosine_query_pos)\n",
    "    true_positives = np.sum(cosine_query_pos >= threshold)\n",
    "    tpr = true_positives / true_distances_count\n",
    "\n",
    "    return threshold, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b3ac47f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1751604644100,
     "user": {
      "displayName": "Павел Быченко",
      "userId": "16450265071672722365"
     },
     "user_tz": -300
    },
    "id": "b3ac47f5",
    "outputId": "e90ae9af-2e78-403b-dcf3-1c4eb9b9ea4e"
   },
   "outputs": [],
   "source": [
    "test_thr = []\n",
    "test_tpr = []\n",
    "for fpr in [0.5, 0.3, 0.1]:\n",
    "  x, y = compute_ir(test_cosine_query_pos, test_cosine_query_neg,\n",
    "                    test_cosine_query_distractors, fpr=fpr)\n",
    "  test_thr.append(x)\n",
    "  test_tpr.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "3a9ea1e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "error",
     "timestamp": 1751604477117,
     "user": {
      "displayName": "Павел Быченко",
      "userId": "16450265071672722365"
     },
     "user_tz": -300
    },
    "id": "3a9ea1e1",
    "outputId": "20d51551-bf53-4048-8313-0445a68a9987"
   },
   "outputs": [],
   "source": [
    "true_thr = [-0.011982733001947084, 0.3371426578637511, 0.701307100338029]\n",
    "assert np.allclose(np.array(test_thr), np.array(true_thr)), \"A mistake in computing threshold\"\n",
    "true_tpr = [0.75, 0.5, 0.5]\n",
    "assert np.allclose(np.array(test_tpr), np.array(true_tpr)), \"A mistake in computing tpr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "H465z6xMBk9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1751609151059,
     "user": {
      "displayName": "Павел Быченко",
      "userId": "16450265071672722365"
     },
     "user_tz": -300
    },
    "id": "H465z6xMBk9d",
    "outputId": "33ce64db-2914-44ab-ee66-7788f242dcde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2937, 9091, 6558, 4257, 8189, 3010, 1672, 4620, 6080, 6284, 6340,\n",
       "            3315, 8372, 5382, 4706, 3328, 7573,  565, 8861, 6417, 4436, 2844,\n",
       "              17, 4836, 1258, 1305, 5951,  953, 8984, 7932],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = ''\n",
    "identity_csv = f\"{DIR}identity_CelebA.csv\"\n",
    "identity_df = pd.read_csv(identity_csv)\n",
    "\n",
    "filtered_csv = f\"{DIR}identity_CelebA_mini.csv\"\n",
    "filtered_df = pd.read_csv(filtered_csv)\n",
    "\n",
    "# identity_df содержит все данные, filtered_df содержит данные, которые использовались при обучении/валидации моделей\n",
    "# вычислим фрейм, который является разницей identity_df и filtered_df\n",
    "other_df = identity_df = identity_df[~identity_df['id'].isin(filtered_df['id'])]\n",
    "# найдем 1530 наиболее часто встречающихся в other_df личностей: 30 личностей для query, 1500 для distractors\n",
    "top_other_df_ids = other_df['id'].value_counts().head(1530).index\n",
    "# разобьем их на query и distractors\n",
    "query_ids = top_other_df_classes[0:30]\n",
    "distractor_ids = top_other_df_classes[30:]\n",
    "query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "8sPS-h2yCs7-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1751609173917,
     "user": {
      "displayName": "Павел Быченко",
      "userId": "16450265071672722365"
     },
     "user_tz": -300
    },
    "id": "8sPS-h2yCs7-",
    "outputId": "ad27d91b-b954-4704-f5b7-20cf19b8e5f5"
   },
   "outputs": [],
   "source": [
    "# функция, которая для заданных ids возвращает images_per_id картинок для каждого id\n",
    "def get_images_for_ids(df, target_ids, images_per_id=3, shuffle=True):\n",
    "    # Отфильтруем только строки с нужными ids\n",
    "    df_query = df[df['id'].isin(target_ids)]\n",
    "    selected_images = []\n",
    "\n",
    "    # Для каждого id выбираем images_per_label картинок\n",
    "    for target_id in target_ids:\n",
    "        images = df_query[df_query['id'] == target_id]['image_id'].tolist()\n",
    "        # добавляем случайности\n",
    "        if shuffle:\n",
    "            random.shuffle(images)  \n",
    "        selected_images.extend(images[:images_per_id])\n",
    "\n",
    "    return selected_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "257602e1",
   "metadata": {
    "id": "257602e1",
    "outputId": "2735822b-33a7-4a56-9fa9-e589c6b8e933"
   },
   "outputs": [],
   "source": [
    "# получим имена картинок из query_ids, по 3 картинки на id\n",
    "query_images = get_images_for_ids(other_df, query_ids, 3)\n",
    "# сразу вычислим ту часть other_df, которая соответствует query_images. Она понадобится дальше\n",
    "query_df = other_df[other_df['image_id'].isin(query_images)]\n",
    "\n",
    "# аналогично для distractor, только берем 1 картинку на id\n",
    "distractor_images = get_images_for_ids(other_df, distractor_ids, 1)\n",
    "distractor_df = other_df[other_df['image_id'].isin(distractor_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "6454d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# небольшая проверка, что все сделано правильно\n",
    "assert sorted(query_df['id'].value_counts().index) == sorted(query_ids)\n",
    "assert sorted(distractor_df['id'].value_counts().index) == sorted(distractor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# папка для картинок для irm\n",
    "images_for_irm = 'images_for_irm'\n",
    "os.makedirs(images_for_irm, exist_ok=True)\n",
    "\n",
    "# тут лежат оригинальные картинки из полного датасета, который я не прикладывал в гитхаб, но указал ссылку по которой можно скачать zip архив\n",
    "SOURCE_IMAGES_DIR = 'img_align_celeba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e50642c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# та же функция для переноса картинок из одной папки в другую, что и ноутбуках ранее\n",
    "def move_files(source_dir, dest_dir, names):\n",
    "    for name in names:\n",
    "        src = os.path.join(source_dir, name)\n",
    "        dest = os.path.join(dest_dir, name)\n",
    "        shutil.copy(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "4c1562a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(SOURCE_IMAGES_DIR, images_for_irm, query_images)\n",
    "move_files(SOURCE_IMAGES_DIR, images_for_irm, distractor_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "ef3b3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# модели обучались на картинках, обработанных определенным образом, поэтому чтобы получить эмбединги с их помощью\n",
    "# нужно картинки также преобразовать\n",
    "\n",
    "# функция для получаения эмбедингов из модели и списка имен картинок\n",
    "def compute_embeddings(model, images_list):\n",
    "    '''\n",
    "    compute embeddings from the trained model for list of images.\n",
    "    params:\n",
    "    model: trained nn model that takes images and outputs embeddings\n",
    "    images_list: list of images paths to compute embeddings for\n",
    "    output:\n",
    "    list: list of model embeddings. Each embedding corresponds to images\n",
    "          names from images_list\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    tensors = []\n",
    "    for img in images_list:\n",
    "        image_path = f'{images_for_irm}/{img}'\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        img = transform(img)\n",
    "        tensors.append(img)\n",
    "    \n",
    "    tensors = torch.stack(tensors)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(tensors)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "6ae34714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{17: ['003677.jpg', '042035.jpg', '130154.jpg'],\n",
       " 565: ['029841.jpg', '081350.jpg', '142460.jpg'],\n",
       " 953: ['162781.jpg', '166102.jpg', '167342.jpg'],\n",
       " 1258: ['096620.jpg', '117114.jpg', '117857.jpg'],\n",
       " 1305: ['006704.jpg', '013951.jpg', '153872.jpg'],\n",
       " 1672: ['036072.jpg', '062136.jpg', '095561.jpg'],\n",
       " 2844: ['032283.jpg', '080003.jpg', '082131.jpg'],\n",
       " 2937: ['024291.jpg', '046844.jpg', '105287.jpg'],\n",
       " 3010: ['026591.jpg', '108313.jpg', '151531.jpg'],\n",
       " 3315: ['166350.jpg', '168085.jpg', '180564.jpg'],\n",
       " 3328: ['119065.jpg', '127861.jpg', '162723.jpg'],\n",
       " 4257: ['068540.jpg', '079481.jpg', '117559.jpg'],\n",
       " 4436: ['029313.jpg', '039714.jpg', '070312.jpg'],\n",
       " 4620: ['023427.jpg', '047966.jpg', '129660.jpg'],\n",
       " 4706: ['174872.jpg', '178932.jpg', '181849.jpg'],\n",
       " 4836: ['003676.jpg', '007532.jpg', '099610.jpg'],\n",
       " 5382: ['005723.jpg', '016781.jpg', '148216.jpg'],\n",
       " 5951: ['011214.jpg', '088187.jpg', '157970.jpg'],\n",
       " 6080: ['193726.jpg', '200098.jpg', '200532.jpg'],\n",
       " 6284: ['198278.jpg', '199628.jpg', '200009.jpg'],\n",
       " 6340: ['071512.jpg', '127225.jpg', '136342.jpg'],\n",
       " 6417: ['046053.jpg', '059203.jpg', '059437.jpg'],\n",
       " 6558: ['017667.jpg', '047127.jpg', '124027.jpg'],\n",
       " 7573: ['190369.jpg', '190603.jpg', '198399.jpg'],\n",
       " 7932: ['030915.jpg', '048256.jpg', '119415.jpg'],\n",
       " 8189: ['023545.jpg', '066053.jpg', '084707.jpg'],\n",
       " 8372: ['024015.jpg', '043733.jpg', '047187.jpg'],\n",
       " 8861: ['185725.jpg', '193589.jpg', '198577.jpg'],\n",
       " 8984: ['183986.jpg', '186826.jpg', '190947.jpg'],\n",
       " 9091: ['015043.jpg', '100873.jpg', '156470.jpg']}"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чтобы использовать написанные выше функции, нужно вычислить словари, где ключ - это id,\n",
    "# а value - массив картинок соответствующий этому id\n",
    "query_dict = query_df.groupby('id')['image_id'].apply(list).to_dict()\n",
    "distractor_dict = distractor_df.groupby('id')['image_id'].apply(list).to_dict()\n",
    "query_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "d320cfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# грузим все имеющиеся модели\n",
    "from torchvision import models\n",
    "\n",
    "cross_entropy_classificator = models.resnet18(pretrained=True)\n",
    "cross_entropy_classificator.fc = nn.Linear(512, 200)\n",
    "cross_entropy_classificator.load_state_dict(torch.load('best_classification_model_with_cross_entropy.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "arc_face_classificator = models.resnet18(pretrained=True)\n",
    "arc_face_classificator.fc = nn.Identity()\n",
    "arc_face_classificator.load_state_dict(torch.load(\"best_classification_model_with_arc_face.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "embedding_size = 256\n",
    "triplet_loss_classificator = models.resnet18(pretrained=True)\n",
    "triplet_loss_classificator.fc = nn.Sequential(\n",
    "    nn.Linear(512, embedding_size),\n",
    "    nn.BatchNorm1d(embedding_size),\n",
    "    nn.ReLU()\n",
    ")\n",
    "triplet_loss_classificator.load_state_dict(torch.load('best_classification_model_with_triplet_loss.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "semi_hard_mining_triplet_loss_classificator = models.resnet18(pretrained=True)\n",
    "semi_hard_mining_triplet_loss_classificator.fc = nn.Sequential(\n",
    "    nn.Linear(512, embedding_size),\n",
    "    nn.BatchNorm1d(embedding_size),\n",
    "    nn.ReLU()\n",
    ")\n",
    "semi_hard_mining_triplet_loss_classificator.load_state_dict(torch.load('best_classification_model_with_semi_hard_mining_triplet_loss.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "0bf3372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для объединения всех предварительных подсчетов и вычисления ir для заданной модели и заданного fpr\n",
    "def get_thr_tpr_for_model(model, fpr):\n",
    "    model_query_embeddings = compute_embeddings(model, query_images)\n",
    "    model_distractor_embeddings = compute_embeddings(model, distractor_images)\n",
    "    model_query_pos = compute_cosine_query_pos(query_dict, query_images, model_query_embeddings)\n",
    "    model_query_neg = compute_cosine_query_neg(query_dict, query_images, model_query_embeddings)\n",
    "    model_query_distractors = compute_cosine_query_distractors(model_query_embeddings, model_distractor_embeddings)\n",
    "    \n",
    "    return compute_ir(model_query_pos, model_query_neg, model_query_distractors, fpr=fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "73eca0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_entropy_classificator_thr [0.8956775069236755, 0.9335075616836548, 0.9478614330291748, 0.9572196006774902]\n",
      "cross_entropy_classificator_tpr [0.9333333333333333, 0.5888888888888889, 0.37777777777777777, 0.26666666666666666]\n",
      "\n",
      "arc_face_classificator_thr [0.6451953053474426, 0.706713080406189, 0.739459753036499, 0.7664155960083008]\n",
      "arc_face_classificator_tpr [0.8666666666666667, 0.6111111111111112, 0.4666666666666667, 0.3]\n",
      "\n",
      "triplet_loss_classificator_thr [0.7015546560287476, 0.858610987663269, 0.9078783392906189, 0.9360910058021545]\n",
      "triplet_loss_classificator_tpr [0.8777777777777778, 0.6, 0.43333333333333335, 0.24444444444444444]\n",
      "\n",
      "semi_hard_mining_triplet_loss_classificator_thr [0.3968157470226288, 0.7473957538604736, 0.8678941130638123, 0.929268479347229]\n",
      "semi_hard_mining_triplet_loss_classificator_tpr [0.9111111111111111, 0.5111111111111111, 0.34444444444444444, 0.17777777777777778]\n"
     ]
    }
   ],
   "source": [
    "# посчитаем и выведем метрики всех моделей для значений fpr = [0.5, 0.2, 0.1, 0.05].\n",
    "cross_entropy_classificator_thr = []\n",
    "cross_entropy_classificator_tpr = []\n",
    "for fpr in [0.5, 0.2, 0.1, 0.05]:\n",
    "    x, y = get_thr_tpr_for_model(cross_entropy_classificator, fpr=fpr)\n",
    "    cross_entropy_classificator_thr.append(x)\n",
    "    cross_entropy_classificator_tpr.append(y)\n",
    "print('cross_entropy_classificator_thr', cross_entropy_classificator_thr)\n",
    "print('cross_entropy_classificator_tpr', cross_entropy_classificator_tpr)\n",
    "print()\n",
    "\n",
    "arc_face_classificator_thr = []\n",
    "arc_face_classificator_tpr = []\n",
    "for fpr in [0.5, 0.2, 0.1, 0.05]:\n",
    "    x, y = get_thr_tpr_for_model(arc_face_classificator, fpr=fpr)\n",
    "    arc_face_classificator_thr.append(x)\n",
    "    arc_face_classificator_tpr.append(y)\n",
    "print('arc_face_classificator_thr', arc_face_classificator_thr)\n",
    "print('arc_face_classificator_tpr', arc_face_classificator_tpr)\n",
    "print()\n",
    "\n",
    "triplet_loss_classificator_thr = []\n",
    "triplet_loss_classificator_tpr = []\n",
    "for fpr in [0.5, 0.2, 0.1, 0.05]:\n",
    "    x, y = get_thr_tpr_for_model(triplet_loss_classificator, fpr=fpr)\n",
    "    triplet_loss_classificator_thr.append(x)\n",
    "    triplet_loss_classificator_tpr.append(y)\n",
    "print('triplet_loss_classificator_thr', triplet_loss_classificator_thr)\n",
    "print('triplet_loss_classificator_tpr', triplet_loss_classificator_tpr)\n",
    "print()\n",
    "\n",
    "semi_hard_mining_triplet_loss_classificator_thr = []\n",
    "semi_hard_mining_triplet_loss_classificator_tpr = []\n",
    "for fpr in [0.5, 0.2, 0.1, 0.05]:\n",
    "    x, y = get_thr_tpr_for_model(semi_hard_mining_triplet_loss_classificator, fpr=fpr)\n",
    "    semi_hard_mining_triplet_loss_classificator_thr.append(x)\n",
    "    semi_hard_mining_triplet_loss_classificator_tpr.append(y)\n",
    "print('semi_hard_mining_triplet_loss_classificator_thr', semi_hard_mining_triplet_loss_classificator_thr)\n",
    "print('semi_hard_mining_triplet_loss_classificator_tpr', semi_hard_mining_triplet_loss_classificator_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c2010",
   "metadata": {},
   "source": [
    "при FPR = 0.5 cross_entropy_classificator даёт лучший tpr, semi_hard_mining_triplet_loss_classificator тоже хорош.\n",
    "при FPR = 0.2 - 0.05 triplet_loss_classificator и arc_face_classificator выглядят предпочтительнее"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
